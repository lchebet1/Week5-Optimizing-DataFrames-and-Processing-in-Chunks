{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7eHGTSKv8TW"
      },
      "source": [
        "# Project Notebook: Optimizing DataFrames and Processing in Chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ7UJlKnwJk3"
      },
      "source": [
        "## 1. Introduction "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUyK3GcBv9mC"
      },
      "source": [
        "In this project, we'll practice working with chunked dataframes and optimizing a dataframe's memory usage. We'll be working with financial lending data from Lending Club, a marketplace for personal loans that matches borrowers with investors. You can read more about the marketplace on its website.\n",
        "\n",
        "The Lending Club's website lists approved loans. Qualified investors can view the borrower's credit score, the purpose of the loan, and other details in the loan applications. Once a lender is ready to back a loan, it selects the amount of money it wants to fund. When the loan amount the borrower requested is fully funded, the borrower receives the money, minus the origination fee that Lending Club charges.\n",
        "\n",
        "We'll be working with a dataset of loans approved from 2007-2011 (https://bit.ly/3H2XVgC). We've already removed the desc column for you to make our system run more quickly.\n",
        "\n",
        "If we read in the entire data set, it will consume about 67 megabytes of memory. Let's imagine that we only have 10 megabytes of memory available throughout this project, so you can practice the concepts you learned in the last two lessons.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. Read in the first five lines from `loans_2007.csv` (https://bit.ly/3H2XVgC) and look for any data quality issues.\n",
        "\n",
        "2. Read in the first 1000 rows from the data set, and calculate the total memory usage for these rows. Increase or decrease the number of rows to converge on a memory usage under five megabytes (to stay on the conservative side)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc9NLSZ5vXPM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7313bf05-066f-4ed7-80e8-78bae558d2e3"
      },
      "source": [
        "from pandas.core.arrays.categorical import no_default\n",
        "# Importing pandas\n",
        "import pandas as pd\n",
        "pd.options.display.max_columns = 99\n",
        "\n",
        "# Your code goes here\n",
        "loans_head = pd.read_csv('https://bit.ly/3H2XVgC', nrows=5,)\n",
        "loans_sample = pd.read_csv('https://bit.ly/3H2XVgC', nrows=1000,)\n",
        "#print(loans_sample.info(memory_usage='deep'))\n",
        "print(f'memory_usage for 1000 rows: {loans_sample.memory_usage(deep=True).sum()/(1024*1024)}')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "memory_usage for 1000 rows: 1.5273666381835938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1000 rows have memory usage of 1.5MB\n",
        "#increament the number of rows in 50 to have memory usage of close to 5MB\n",
        "chunk_memory = 0\n",
        "no_rows =2500\n",
        "while(chunk_memory <5):\n",
        "  no_rows += 50\n",
        "  loans_sample = pd.read_csv('https://bit.ly/3H2XVgC', nrows=no_rows,)\n",
        "  chunk_memory = loans_sample.memory_usage(deep=True).sum()/(1024*1024)\n",
        "\n",
        "print(f'no of rows for just 5MB memory usage: {no_rows-50}')\n",
        "print(f'chunk memory for the max rows {chunk_memory}')\n",
        "no_rows = no_rows-50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMVGQhM4k8JO",
        "outputId": "daac77db-c2d9-4eab-dd71-ed76cd2a0080"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no of rows for just 5MB memory usage: 3250\n",
            "chunk memory for the max rows 5.038409233093262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##chunk size\n",
        "we will be working with chunk size of 3250 rows"
      ],
      "metadata": {
        "id": "c0g4pdPh74qU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StemrBe2wkMj"
      },
      "source": [
        "## 2. Exploring the Data in Chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm4p982Dwor5"
      },
      "source": [
        "Let's familiarize ourselves with the columns to see which ones we can optimize. In the first lesson, we explored column types by reading in the full dataframe. In this project, let's try to understand the column types better while using dataframe chunks.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "For each chunk:\n",
        "* How many columns have a numeric type? \n",
        "* How many have a string type?\n",
        "* How many unique values are there in each string column? How many of the string columns contain values that are less than 50% unique?\n",
        "* Which float columns have no missing values and could be candidates for conversion to the integer type?\n",
        "* Calculate the total memory usage across all of the chunks."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##observation\n",
        "### Each float column has atleast 3 na values for the whole dataset. use a filter columns <100 na values, drop na and convert the float column to integer"
      ],
      "metadata": {
        "id": "BEcfS5iRLrFf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjTUXAnJwzdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "470b596a-e0f6-44ed-c4b8-88ab899fb43f"
      },
      "source": [
        "from pandas.core.tools import numeric\n",
        "# Your code goes here\n",
        "chunk_iter = pd.read_csv('https://bit.ly/3H2XVgC', chunksize=no_rows,)\n",
        "\n",
        "numeric_cols = []\n",
        "float_cols_na = []\n",
        "string_cols = []\n",
        "string_cols_uniqueness = []\n",
        "total_string_cols_uniqueness = pd.Series()\n",
        "chunk_memory_usage = []\n",
        "for chunk in chunk_iter:\n",
        "  #the number of numeric columns for each chunk\n",
        "  #the 'id' column number, numeric string and non-numeric string\n",
        "  chunk['id'] = pd.to_numeric(chunk['id'], errors='coerce') #non-numeric string converted to null values\n",
        "  chunk = chunk.dropna(axis=0, subset=['id']) #drop row where id is missing\n",
        "  chunk_numeric_cols = chunk.select_dtypes(exclude=['object']).shape[1]\n",
        "  numeric_cols.append(chunk_numeric_cols)\n",
        "  \n",
        "  #check for any missing value for each chunk numeric columns\n",
        "  chunk_float_cols = chunk.select_dtypes(include=['float64'])\n",
        "  float_cols_na.append(chunk_float_cols.isnull().sum())\n",
        "\n",
        "  #the number of string columns for each chunk\n",
        "  chunk_string_cols = chunk.select_dtypes(include=['object']).shape[1]\n",
        "  string_cols.append(chunk_string_cols)\n",
        "\n",
        "  #unique values in each string column and check if they are <50% of number of values in the column\n",
        "  chunk_string_cols_unique = chunk.select_dtypes(include=['object']).nunique() #pd.series with unique values per col\n",
        "  chunk_string_cols_count = chunk.select_dtypes(include=['object']).count() #total no of items in col\n",
        "  string_cols_uniqueness.append((100*chunk_string_cols_unique/chunk_string_cols_count)) #add each chunk series to a list\n",
        "\n",
        "  #memory usage for the chunk\n",
        "  chunk_memory_usage.append(chunk.memory_usage(deep=True))\n",
        "\n",
        "#\n",
        "#concatenate the series in the list to a series\n",
        "total_string_cols_uniqueness = pd.concat(string_cols_uniqueness)\n",
        "total_string_cols_uniqueness = total_string_cols_uniqueness.groupby(total_string_cols_uniqueness.index).mean()\n",
        "categorise_string_cols = list((total_string_cols_uniqueness[total_string_cols_uniqueness <50]).index)\n",
        "\n",
        "#combine all numeric columns missing values\n",
        "total_float_cols_na = pd.concat(float_cols_na)\n",
        "total_float_cols_na = total_float_cols_na.groupby(total_float_cols_na.index).sum()\n",
        "zero_na_float_col = list((total_float_cols_na[total_float_cols_na == 0 ]).index)\n",
        "float_col_with_na = list((total_float_cols_na[total_float_cols_na > 0 ]).index)\n",
        "\n",
        "#sum up the memory usage\n",
        "total_memory_usage = pd.concat(chunk_memory_usage).sum()/(1024*1024)\n",
        "\n",
        "print(f'\\nzero na float columns:\\n {zero_na_float_col}')\n",
        "print(f'\\nfloat columns with null value:\\n {float_col_with_na}')\n",
        "print(f'\\nmissing values in dataset:\\n {total_float_cols_na}')\n",
        "print(f'\\nchunks:{len(numeric_cols)} chunks: numeric columns for the chunks: {numeric_cols}')\n",
        "print(f'\\nchunks:{len(string_cols)} chunks: string columns for the chunks: {string_cols}')\n",
        "print(f'\\npercentage of unique values in the columns: \\n {total_string_cols_uniqueness}')\n",
        "print(f'\\ncolumns that can be converted to category dtype: \\n cols no:{len(categorise_string_cols)} : {categorise_string_cols}')\n",
        "print(f'\\nMemory usage: {total_memory_usage}MB')\n",
        "\n",
        "  \n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-e3abb922cbcc>:9: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  total_string_cols_uniqueness = pd.Series()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "zero na float columns:\n",
            " ['collection_recovery_fee', 'dti', 'funded_amnt', 'funded_amnt_inv', 'id', 'installment', 'last_pymnt_amnt', 'loan_amnt', 'member_id', 'out_prncp', 'out_prncp_inv', 'policy_code', 'recoveries', 'revol_bal', 'total_pymnt', 'total_pymnt_inv', 'total_rec_int', 'total_rec_late_fee', 'total_rec_prncp']\n",
            "\n",
            "float columns with null value:\n",
            " ['acc_now_delinq', 'annual_inc', 'chargeoff_within_12_mths', 'collections_12_mths_ex_med', 'delinq_2yrs', 'delinq_amnt', 'inq_last_6mths', 'open_acc', 'pub_rec', 'pub_rec_bankruptcies', 'tax_liens', 'total_acc']\n",
            "\n",
            "missing values in dataset:\n",
            " acc_now_delinq                  29\n",
            "annual_inc                       4\n",
            "chargeoff_within_12_mths       145\n",
            "collection_recovery_fee          0\n",
            "collections_12_mths_ex_med     145\n",
            "delinq_2yrs                     29\n",
            "delinq_amnt                     29\n",
            "dti                              0\n",
            "funded_amnt                      0\n",
            "funded_amnt_inv                  0\n",
            "id                               0\n",
            "inq_last_6mths                  29\n",
            "installment                      0\n",
            "last_pymnt_amnt                  0\n",
            "loan_amnt                        0\n",
            "member_id                        0\n",
            "open_acc                        29\n",
            "out_prncp                        0\n",
            "out_prncp_inv                    0\n",
            "policy_code                      0\n",
            "pub_rec                         29\n",
            "pub_rec_bankruptcies          1365\n",
            "recoveries                       0\n",
            "revol_bal                        0\n",
            "tax_liens                      105\n",
            "total_acc                       29\n",
            "total_pymnt                      0\n",
            "total_pymnt_inv                  0\n",
            "total_rec_int                    0\n",
            "total_rec_late_fee               0\n",
            "total_rec_prncp                  0\n",
            "dtype: int64\n",
            "\n",
            "chunks:14 chunks: numeric columns for the chunks: [31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
            "\n",
            "chunks:14 chunks: string columns for the chunks: [21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21]\n",
            "\n",
            "percentage of unique values in the columns: \n",
            " addr_state              1.972661\n",
            "application_type        0.053547\n",
            "earliest_cr_line       15.107063\n",
            "emp_length              0.597571\n",
            "emp_title              92.650197\n",
            "grade                   0.374830\n",
            "home_ownership          0.196607\n",
            "initial_list_status     0.053547\n",
            "int_rate                3.077309\n",
            "issue_d                 0.403426\n",
            "last_credit_pull_d      3.818873\n",
            "last_pymnt_d            2.809311\n",
            "loan_status             0.162041\n",
            "purpose                 0.709300\n",
            "pymnt_plan              0.055745\n",
            "revol_util             33.460831\n",
            "sub_grade               1.869754\n",
            "term                    0.077724\n",
            "title                  64.636038\n",
            "verification_status     0.110691\n",
            "zip_code               20.026197\n",
            "dtype: float64\n",
            "\n",
            "columns that can be converted to category dtype: \n",
            " cols no:19 : ['addr_state', 'application_type', 'earliest_cr_line', 'emp_length', 'grade', 'home_ownership', 'initial_list_status', 'int_rate', 'issue_d', 'last_credit_pull_d', 'last_pymnt_d', 'loan_status', 'purpose', 'pymnt_plan', 'revol_util', 'sub_grade', 'term', 'verification_status', 'zip_code']\n",
            "\n",
            "Memory usage: 65.37709045410156MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BtAVP8fw3OH"
      },
      "source": [
        "## 3. Optimizing String Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkWArHAhw_bw"
      },
      "source": [
        "We can achieve the greatest memory improvements by converting the string columns to a numeric type. Let's convert all of the columns where the values are less than 50% unique to the category type, and the columns that contain numeric values to the `float` type.\n",
        "\n",
        "While working with dataframe chunks:\n",
        "* Determine which string columns you can convert to a numeric type if you clean them. For example, the `int_rate` column is only a string because of the % sign at the end.\n",
        "* Determine which columns have a few unique values and convert them to the category type. For example, you may want to convert the grade and `sub_grade` columns.\n",
        "Based on your conclusions, perform the necessary type changes across all chunks. * Calculate the total memory footprint, and compare it with the previous one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH0tcQlpxG9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38461fbb-8e4d-44f5-a6b2-3ed6d0c61737"
      },
      "source": [
        "# Your code goes here\n",
        "chunk_iter = pd.read_csv('https://bit.ly/3H2XVgC', chunksize=no_rows,)\n",
        "#columns to be categorised, remove columns to be converted float dtype\n",
        "categorise_string_cols.remove('int_rate')\n",
        "categorise_string_cols.remove('revol_util')\n",
        "\n",
        "string_optimised_mem_usage = []\n",
        "for chunk in chunk_iter:\n",
        "  #cols 'int_rate' and 'revol_util' can be converted to float by removing %\n",
        "  chunk['int_rate'] = chunk['int_rate'].str.replace('%', '').astype('float')\n",
        "  chunk['revol_util'] = chunk['revol_util'].str.replace('%', '').astype('float')\n",
        "  chunk['int_rate'] = pd.to_numeric(chunk['int_rate'], downcast='float')\n",
        "  chunk['revol_util'] = pd.to_numeric(chunk['revol_util'], downcast='float')\n",
        "  #categorise columns with unique values <50% of total items in the column\n",
        "  chunk[categorise_string_cols] = chunk[categorise_string_cols].astype('category')\n",
        "\n",
        "  #memory usage of the chunk optimised strings columns\n",
        "  string_optimised_mem_usage.append(chunk.memory_usage(deep=True))\n",
        "  \n",
        "\n",
        "#sum up the memory usage\n",
        "total_string_optimised_mem = pd.concat(string_optimised_mem_usage).sum()/(1024*1024)\n",
        "print(f'\\nmem usage with string columns optimisation\\n {total_string_optimised_mem}MB')\n",
        "print(f'memory usage improved {total_memory_usage/total_string_optimised_mem}times')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "mem usage with string columns optimisation\n",
            " 18.75536346435547MB\n",
            "memory usage improved 3.485781044891537times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Observation:\n",
        "### memory utilisation after string columns optimisation reduced from 65MB to 18MB"
      ],
      "metadata": {
        "id": "IEa0-JyFS9fC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22oYzgXnxIcV"
      },
      "source": [
        "## 4. Optimizing Numeric Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv5C20YrxPID"
      },
      "source": [
        "It looks like we were able to realize some powerful memory savings by converting to the category type and converting string columns to numeric ones.\n",
        "\n",
        "Now let's optimize the numeric columns using the `pandas.to_numeric()` function.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "While working with dataframe chunks:\n",
        "* Identify float columns that contain missing values, and that we can convert to a more space efficient subtype.\n",
        "* Identify float columns that don't contain any missing values, and that we can convert to the integer type because they represent whole numbers.\n",
        "* Based on your conclusions, perform the necessary type changes across all chunks.\n",
        "* Calculate the total memory footprint and compare it with the previous one.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code goes here\n",
        "\n",
        "chunk_iter = pd.read_csv('https://bit.ly/3H2XVgC', chunksize=no_rows,)\n",
        "\n",
        "numeric_optimised_mem_usage = []\n",
        "total_int_na = [] \n",
        "\n",
        "#iterate through df columns and downcast dtype\n",
        "def mem_optimisation(df, columns, convert_dtype):\n",
        "  for c in columns:\n",
        "    df[c] = pd.to_numeric(df[c], downcast=convert_dtype, errors='coerce')\n",
        "  return df\n",
        "\n",
        "for chunk in chunk_iter:\n",
        "  #convert the id column object to float and remove null value\n",
        "  chunk['id'] = pd.to_numeric(chunk['id'], errors='coerce') #non-numeric string converted to null values\n",
        "  chunk = chunk.dropna(axis=0, subset=['id']) #drop row where id is missing\n",
        "  \n",
        "  #convert the float columns with zero null value to int\n",
        "  chunk = mem_optimisation(chunk, zero_na_float_col, 'integer')\n",
        "  chunk = mem_optimisation(chunk, float_col_with_na, 'float')\n",
        "\n",
        "  #memory usage of the chunk optimised strings columns\n",
        "  numeric_optimised_mem_usage.append(chunk.memory_usage(deep=True))\n",
        "\n",
        "#sum up the memory usage\n",
        "total_numeric_optimised_mem = pd.concat(numeric_optimised_mem_usage).sum()/(1024*1024)\n",
        "print(f'\\nmem usage with numeric columns optimisation\\n {total_numeric_optimised_mem}MB')\n",
        "print(f'memory usage improved {total_memory_usage/total_numeric_optimised_mem}times')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxhvZultwk-1",
        "outputId": "648cb26c-3fb0-475c-ca83-54a657faab46"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "mem usage with numeric columns optimisation\n",
            " 62.038339614868164MB\n",
            "memory usage improved 1.0538175402494692times\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-09892fddb9bd>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[c] = pd.to_numeric(df[c], downcast=convert_dtype, errors='coerce')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMJVaj_kxj42"
      },
      "source": [
        "## Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0RL3BzexlkW"
      },
      "source": [
        "We've practiced optimizing a dataframe's memory footprint and working with dataframe chunks. Here's an idea for some next steps:\n",
        "\n",
        "Create a function that automates as much of the work you just did as possible, so that you could use it on other Lending Club data sets. This function should:\n",
        "\n",
        "* Determine the optimal chunk size based on the memory constraints you provide.\n",
        "\n",
        "* Determine which string columns can be converted to numeric ones by removing the `%` character.\n",
        "\n",
        "* Determine which numeric columns can be converted to more space efficient representations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hvuUNzPx1zy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee2ae5b1-5802-42dd-f1cb-05218d6ed06e"
      },
      "source": [
        "# Your code goes here\n",
        "#Determine the optimal chunk size based on the memory constraints you provide.  \n",
        "def chunk_size(csv_file, available_mem, start_chunk_size, chunk_step):\n",
        "  no_rows = start_chunk_size\n",
        "  chunk_memory =0\n",
        "  while(chunk_memory < available_mem):\n",
        "    no_rows += chunk_step\n",
        "    chunk = pd.read_csv(csv_file, nrows=no_rows,)\n",
        "    chunk_memory = chunk.memory_usage(deep=True).sum()/(1024*1024)\n",
        "    \n",
        "  #reduce by 1\n",
        "  #no_rows -=chunk_step\n",
        "  #get the chunks\n",
        "  #chunk_iter = pd.read_csv(csv_file, chunk_size=no_rows,)#\n",
        "\n",
        "  return no_rows-chunk_step\n",
        "\n",
        "no_rows = chunk_size('https://bit.ly/3H2XVgC', 5, 3000, 50)\n",
        "print(no_rows)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.utils.py3compat import iteritems\n",
        "#Determine which string columns can be converted to numeric ones by removing the % character.\n",
        "def string_to_numeric(df, ch):\n",
        "  columns_with_ch = []\n",
        "  for r,c in df.iteritems():\n",
        "    # ch_found = c.str.contains(ch)\n",
        "    # print(ch_found)\n",
        "    #check if column has % character, remove it and try to convert to numeric, \n",
        "    #if it had only % character, no error raised but if other string character error will be raised\n",
        "    if  c.str.contains(ch).any():\n",
        "      try:\n",
        "        c = c.str.replace('%', '')\n",
        "        c = pd.to_numeric(c, errors='raise')\n",
        "        columns_with_ch.append(r)\n",
        "      except(ValueError):\n",
        "        #dont add columns to the list\n",
        "        continue\n",
        "  print(columns_with_ch)\n",
        "\n",
        "loans = pd.read_csv('https://bit.ly/3H2XVgC')\n",
        "string_cols = loans.select_dtypes(include='object')\n",
        "string_to_numeric(string_cols, '%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_YH8WwJ6mF4",
        "outputId": "a1e0cd0a-f57d-402f-d861-f9f5a3f373aa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['int_rate', 'revol_util']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Determine which numeric columns can be converted to more space efficient representations\n",
        "#numeric cols can be int64(no missing value) or float64(allow missing value)\n",
        "def numeric_cols_optimise(df): \n",
        "  #select float type columns and check the number of missing value per columns\n",
        "  numeric_cols = df.select_dtypes(exclude=['object'])\n",
        "  cols_missing_value = numeric_cols.isnull().sum()\n",
        "  #float columns with no null values can be converted to int type\n",
        "  int_cols = list(cols_missing_value[cols_missing_value == 0].index)\n",
        "  float_cols = list(cols_missing_value[cols_missing_value >0].index)\n",
        "\n",
        "  return int_cols, float_cols\n",
        "\n",
        "#\n",
        "loans = pd.read_csv('https://bit.ly/3H2XVgC')\n",
        "loans['id']=pd.to_numeric(loans['id'], errors='coerce')\n",
        "loans.dropna(axis=0, subset=['id'], inplace=True)\n",
        "\n",
        "x,y = numeric_cols_optimise(loans)\n",
        "print(f'int columns: {x}')\n",
        "print(f'float columns: {y}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIzH6HjBK-74",
        "outputId": "f0fdd676-ec40-481c-c64e-0dafe289efa6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "int columns: ['id', 'member_id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'installment', 'dti', 'revol_bal', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', 'last_pymnt_amnt', 'policy_code']\n",
            "float columns: ['annual_inc', 'delinq_2yrs', 'inq_last_6mths', 'open_acc', 'pub_rec', 'total_acc', 'collections_12_mths_ex_med', 'acc_now_delinq', 'chargeoff_within_12_mths', 'delinq_amnt', 'pub_rec_bankruptcies', 'tax_liens']\n"
          ]
        }
      ]
    }
  ]
}